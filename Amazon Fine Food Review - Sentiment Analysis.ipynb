{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18de164",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae73928",
   "metadata": {},
   "source": [
    "# Read Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00782ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e796b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data - first 1000 rows\n",
    "data = pd.read_csv(\"Reviews.csv\", nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb84c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36404705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check no. of rows & columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d7086",
   "metadata": {},
   "source": [
    "# Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d417c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I have bought several of the Vitality canned d...\n",
       "1    Product arrived labeled as Jumbo Salted Peanut...\n",
       "2    This is a confection that has been around a fe...\n",
       "3    If you are looking for the secret ingredient i...\n",
       "4    Great taffy at a great price.  There was a wid...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select Text column\n",
    "data_t = data['Text']\n",
    "data_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "495ed4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# check no. of duplicates in Text column\n",
    "data_t_duplicates = data_t.duplicated()\n",
    "print(data_t_duplicates.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125e48f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates in Text column\n",
    "data_rdup = data_t.drop_duplicates()\n",
    "data_rdup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67cecdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twizzlers, Strawberry my childhood favorite candy, made in Lancaster Pennsylvania by Y & S Candies, Inc. one of the oldest confectionery Firms in the United States, now a Subsidiary of the Hershey Company, the Company was established in 1845 as Young and Smylie, they also make Apple Licorice Twists, Green Color and Blue Raspberry Licorice Twists, I like them all<br /><br />I keep it in a dry cool place because is not recommended it to put it in the fridge. According to the Guinness Book of Records, the longest Licorice Twist ever made measured 1.200 Feet (370 M) and weighted 100 Pounds (45 Kg) and was made by Y & S Candies, Inc. This Record-Breaking Twist became a Guinness World Record on July 19, 1998. This Product is Kosher! Thank You\n"
     ]
    }
   ],
   "source": [
    "# check any row\n",
    "print(data_rdup[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdb8b1",
   "metadata": {},
   "source": [
    "# Data Cleaning & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8601c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901a76f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.strip() is to remove white spaces after you do substitution\n",
    "def get_cleaned_textdata(sentence):\n",
    "    modified_sentence = re.sub(r'<.*?>',' ', sentence)\n",
    "    modified_sentence = ''.join([i if i not in string.punctuation else ' ' for i in modified_sentence])\n",
    "    modified_sentence = re.sub(r'\\d+', ' ', modified_sentence)\n",
    "    modified_sentence = re.sub(r'\\s+', ' ', modified_sentence)\n",
    "    modified_sentence = re.sub(r'also', ' ', modified_sentence)\n",
    "    modified_sentence = modified_sentence.strip().lower()\n",
    "    return modified_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3515d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdupclean = data_rdup.apply(get_cleaned_textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b890f71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twizzlers, Strawberry my childhood favorite candy, made in Lancaster Pennsylvania by Y & S Candies, Inc. one of the oldest confectionery Firms in the United States, now a Subsidiary of the Hershey Company, the Company was established in 1845 as Young and Smylie, they also make Apple Licorice Twists, Green Color and Blue Raspberry Licorice Twists, I like them all<br /><br />I keep it in a dry cool place because is not recommended it to put it in the fridge. According to the Guinness Book of Records, the longest Licorice Twist ever made measured 1.200 Feet (370 M) and weighted 100 Pounds (45 Kg) and was made by Y & S Candies, Inc. This Record-Breaking Twist became a Guinness World Record on July 19, 1998. This Product is Kosher! Thank You \n",
      "\n",
      "twizzlers strawberry my childhood favorite candy made in lancaster pennsylvania by y s candies inc one of the oldest confectionery firms in the united states now a subsidiary of the hershey company the company was established in as young and smylie they   make apple licorice twists green color and blue raspberry licorice twists i like them all i keep it in a dry cool place because is not recommended it to put it in the fridge according to the guinness book of records the longest licorice twist ever made measured feet m and weighted pounds kg and was made by y s candies inc this record breaking twist became a guinness world record on july this product is kosher thank you\n"
     ]
    }
   ],
   "source": [
    "# check result of data cleaning\n",
    "print(data_rdup[18],\"\\n\")\n",
    "print(data_rdupclean[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd5e64",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab1b7b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qeme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eae858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64cfe56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_token = data_rdupclean.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149c30f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twizzlers strawberry my childhood favorite candy made in lancaster pennsylvania by y s candies inc one of the oldest confectionery firms in the united states now a subsidiary of the hershey company the company was established in as young and smylie they   make apple licorice twists green color and blue raspberry licorice twists i like them all i keep it in a dry cool place because is not recommended it to put it in the fridge according to the guinness book of records the longest licorice twist ever made measured feet m and weighted pounds kg and was made by y s candies inc this record breaking twist became a guinness world record on july this product is kosher thank you \n",
      "\n",
      "['twizzlers', 'strawberry', 'my', 'childhood', 'favorite', 'candy', 'made', 'in', 'lancaster', 'pennsylvania', 'by', 'y', 's', 'candies', 'inc', 'one', 'of', 'the', 'oldest', 'confectionery', 'firms', 'in', 'the', 'united', 'states', 'now', 'a', 'subsidiary', 'of', 'the', 'hershey', 'company', 'the', 'company', 'was', 'established', 'in', 'as', 'young', 'and', 'smylie', 'they', 'make', 'apple', 'licorice', 'twists', 'green', 'color', 'and', 'blue', 'raspberry', 'licorice', 'twists', 'i', 'like', 'them', 'all', 'i', 'keep', 'it', 'in', 'a', 'dry', 'cool', 'place', 'because', 'is', 'not', 'recommended', 'it', 'to', 'put', 'it', 'in', 'the', 'fridge', 'according', 'to', 'the', 'guinness', 'book', 'of', 'records', 'the', 'longest', 'licorice', 'twist', 'ever', 'made', 'measured', 'feet', 'm', 'and', 'weighted', 'pounds', 'kg', 'and', 'was', 'made', 'by', 'y', 's', 'candies', 'inc', 'this', 'record', 'breaking', 'twist', 'became', 'a', 'guinness', 'world', 'record', 'on', 'july', 'this', 'product', 'is', 'kosher', 'thank', 'you']\n"
     ]
    }
   ],
   "source": [
    "# check result of tokenization\n",
    "print(data_rdupclean[18],\"\\n\")\n",
    "print(data_token[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c8946",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d804ca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Qeme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379a74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db4dfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    output = []\n",
    "    for i in text:\n",
    "        if i not in stopwords:\n",
    "            output.append(i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59ee042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_xstopwords = data_token.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6661962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twizzlers', 'strawberry', 'my', 'childhood', 'favorite', 'candy', 'made', 'in', 'lancaster', 'pennsylvania', 'by', 'y', 's', 'candies', 'inc', 'one', 'of', 'the', 'oldest', 'confectionery', 'firms', 'in', 'the', 'united', 'states', 'now', 'a', 'subsidiary', 'of', 'the', 'hershey', 'company', 'the', 'company', 'was', 'established', 'in', 'as', 'young', 'and', 'smylie', 'they', 'make', 'apple', 'licorice', 'twists', 'green', 'color', 'and', 'blue', 'raspberry', 'licorice', 'twists', 'i', 'like', 'them', 'all', 'i', 'keep', 'it', 'in', 'a', 'dry', 'cool', 'place', 'because', 'is', 'not', 'recommended', 'it', 'to', 'put', 'it', 'in', 'the', 'fridge', 'according', 'to', 'the', 'guinness', 'book', 'of', 'records', 'the', 'longest', 'licorice', 'twist', 'ever', 'made', 'measured', 'feet', 'm', 'and', 'weighted', 'pounds', 'kg', 'and', 'was', 'made', 'by', 'y', 's', 'candies', 'inc', 'this', 'record', 'breaking', 'twist', 'became', 'a', 'guinness', 'world', 'record', 'on', 'july', 'this', 'product', 'is', 'kosher', 'thank', 'you'] \n",
      "\n",
      "['twizzlers', 'strawberry', 'childhood', 'favorite', 'candy', 'made', 'lancaster', 'pennsylvania', 'candies', 'inc', 'one', 'oldest', 'confectionery', 'firms', 'united', 'states', 'subsidiary', 'hershey', 'company', 'company', 'established', 'young', 'smylie', 'make', 'apple', 'licorice', 'twists', 'green', 'color', 'blue', 'raspberry', 'licorice', 'twists', 'like', 'keep', 'dry', 'cool', 'place', 'recommended', 'put', 'fridge', 'according', 'guinness', 'book', 'records', 'longest', 'licorice', 'twist', 'ever', 'made', 'measured', 'feet', 'weighted', 'pounds', 'kg', 'made', 'candies', 'inc', 'record', 'breaking', 'twist', 'became', 'guinness', 'world', 'record', 'july', 'product', 'kosher', 'thank']\n"
     ]
    }
   ],
   "source": [
    "# check result of stopwords removal\n",
    "print(data_token[18],\"\\n\")\n",
    "print(data_xstopwords[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a3946",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8dc1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3000712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2295497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stemming(text):\n",
    "    stem_text = []\n",
    "    for word in text:\n",
    "        stemmed_word = porter_stemmer.stem(word)\n",
    "        stem_text.append(stemmed_word)\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74e89371",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_porterstem = data_xstopwords.apply(porter_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "985c29bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twizzlers', 'strawberry', 'childhood', 'favorite', 'candy', 'made', 'lancaster', 'pennsylvania', 'candies', 'inc', 'one', 'oldest', 'confectionery', 'firms', 'united', 'states', 'subsidiary', 'hershey', 'company', 'company', 'established', 'young', 'smylie', 'make', 'apple', 'licorice', 'twists', 'green', 'color', 'blue', 'raspberry', 'licorice', 'twists', 'like', 'keep', 'dry', 'cool', 'place', 'recommended', 'put', 'fridge', 'according', 'guinness', 'book', 'records', 'longest', 'licorice', 'twist', 'ever', 'made', 'measured', 'feet', 'weighted', 'pounds', 'kg', 'made', 'candies', 'inc', 'record', 'breaking', 'twist', 'became', 'guinness', 'world', 'record', 'july', 'product', 'kosher', 'thank'] \n",
      "\n",
      "\n",
      "['twizzler', 'strawberri', 'childhood', 'favorit', 'candi', 'made', 'lancast', 'pennsylvania', 'candi', 'inc', 'one', 'oldest', 'confectioneri', 'firm', 'unit', 'state', 'subsidiari', 'hershey', 'compani', 'compani', 'establish', 'young', 'smyli', 'make', 'appl', 'licoric', 'twist', 'green', 'color', 'blue', 'raspberri', 'licoric', 'twist', 'like', 'keep', 'dri', 'cool', 'place', 'recommend', 'put', 'fridg', 'accord', 'guin', 'book', 'record', 'longest', 'licoric', 'twist', 'ever', 'made', 'measur', 'feet', 'weight', 'pound', 'kg', 'made', 'candi', 'inc', 'record', 'break', 'twist', 'becam', 'guin', 'world', 'record', 'juli', 'product', 'kosher', 'thank']\n"
     ]
    }
   ],
   "source": [
    "# check result after Porter stemming\n",
    "print(data_xstopwords[18], \"\\n\\n\")\n",
    "print(data_porterstem[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99893705",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bff6cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Qeme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ca75f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Lemmatizer function from nltk library\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2646680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3335de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = []\n",
    "    for word in text:\n",
    "        lemmatized_word = wordnet_lemmatizer.lemmatize(word)\n",
    "        lemm_text.append(lemmatized_word)\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f2e18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the lemmatizer function to data_porterstem\n",
    "data_lemmatized1 = data_porterstem.apply(lemmatizer)\n",
    "data_lemmatized2 = data_xstopwords.apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a80c73b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twizzler', 'strawberri', 'childhood', 'favorit', 'candi', 'made', 'lancast', 'pennsylvania', 'candi', 'inc', 'one', 'oldest', 'confectioneri', 'firm', 'unit', 'state', 'subsidiari', 'hershey', 'compani', 'compani', 'establish', 'young', 'smyli', 'make', 'appl', 'licoric', 'twist', 'green', 'color', 'blue', 'raspberri', 'licoric', 'twist', 'like', 'keep', 'dri', 'cool', 'place', 'recommend', 'put', 'fridg', 'accord', 'guin', 'book', 'record', 'longest', 'licoric', 'twist', 'ever', 'made', 'measur', 'feet', 'weight', 'pound', 'kg', 'made', 'candi', 'inc', 'record', 'break', 'twist', 'becam', 'guin', 'world', 'record', 'juli', 'product', 'kosher', 'thank'] \n",
      "\n",
      "\n",
      "['twizzler', 'strawberri', 'childhood', 'favorit', 'candi', 'made', 'lancast', 'pennsylvania', 'candi', 'inc', 'one', 'oldest', 'confectioneri', 'firm', 'unit', 'state', 'subsidiari', 'hershey', 'compani', 'compani', 'establish', 'young', 'smyli', 'make', 'appl', 'licoric', 'twist', 'green', 'color', 'blue', 'raspberri', 'licoric', 'twist', 'like', 'keep', 'dri', 'cool', 'place', 'recommend', 'put', 'fridg', 'accord', 'guin', 'book', 'record', 'longest', 'licoric', 'twist', 'ever', 'made', 'measur', 'foot', 'weight', 'pound', 'kg', 'made', 'candi', 'inc', 'record', 'break', 'twist', 'becam', 'guin', 'world', 'record', 'juli', 'product', 'kosher', 'thank'] \n",
      "\n",
      "\n",
      "['twizzlers', 'strawberry', 'childhood', 'favorite', 'candy', 'made', 'lancaster', 'pennsylvania', 'candies', 'inc', 'one', 'oldest', 'confectionery', 'firms', 'united', 'states', 'subsidiary', 'hershey', 'company', 'company', 'established', 'young', 'smylie', 'make', 'apple', 'licorice', 'twists', 'green', 'color', 'blue', 'raspberry', 'licorice', 'twists', 'like', 'keep', 'dry', 'cool', 'place', 'recommended', 'put', 'fridge', 'according', 'guinness', 'book', 'records', 'longest', 'licorice', 'twist', 'ever', 'made', 'measured', 'feet', 'weighted', 'pounds', 'kg', 'made', 'candies', 'inc', 'record', 'breaking', 'twist', 'became', 'guinness', 'world', 'record', 'july', 'product', 'kosher', 'thank'] \n",
      "\n",
      "\n",
      "['twizzlers', 'strawberry', 'childhood', 'favorite', 'candy', 'made', 'lancaster', 'pennsylvania', 'candy', 'inc', 'one', 'oldest', 'confectionery', 'firm', 'united', 'state', 'subsidiary', 'hershey', 'company', 'company', 'established', 'young', 'smylie', 'make', 'apple', 'licorice', 'twist', 'green', 'color', 'blue', 'raspberry', 'licorice', 'twist', 'like', 'keep', 'dry', 'cool', 'place', 'recommended', 'put', 'fridge', 'according', 'guinness', 'book', 'record', 'longest', 'licorice', 'twist', 'ever', 'made', 'measured', 'foot', 'weighted', 'pound', 'kg', 'made', 'candy', 'inc', 'record', 'breaking', 'twist', 'became', 'guinness', 'world', 'record', 'july', 'product', 'kosher', 'thank'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check result after Lemmatization\n",
    "print(data_porterstem[18], \"\\n\\n\")\n",
    "print(data_lemmatized1[18], \"\\n\\n\")\n",
    "print(data_xstopwords[18], \"\\n\\n\")\n",
    "print(data_lemmatized2[18], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a09c9",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Convert text data into numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fa4011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import another library\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ff76b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions for calculating the tf, idf and tf-idf\n",
    "def compute_tf(document):\n",
    "    word_count = Counter(document)\n",
    "    tf = {word: count/len(document) for word, count in word_count.items()}\n",
    "    return tf\n",
    "\n",
    "def compute_idf(documents):\n",
    "    N = len(documents)\n",
    "    idf = {}\n",
    "    all_words = set(word for doc in documents for word in doc)\n",
    "    for word in all_words:\n",
    "        count = sum(1 for doc in documents if word in doc)\n",
    "        idf[word] = math.log(N/count)\n",
    "    return idf\n",
    "\n",
    "def compute_tfidf(document, idf):\n",
    "    tfidf = {}\n",
    "    tf = compute_tf(document)\n",
    "    for word, tf_value in tf.items():\n",
    "        tfidf[word] = tf_value * idf[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bddeb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute the tf or the data that already lemmetised\n",
    "tf_data1 = [compute_tf(doc) for doc in data_lemmatized1]\n",
    "tf_data2 = [compute_tf(doc) for doc in data_lemmatized2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0f4bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF df1 Scores:\n",
      "       bought     sever     vital       can       dog      food   product  \\\n",
      "0    0.043478  0.043478  0.043478  0.043478  0.043478  0.043478  0.130435   \n",
      "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.111111   \n",
      "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "992  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "993  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "994  0.000000  0.000000  0.000000  0.000000  0.000000  0.040000  0.000000   \n",
      "995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "996  0.018868  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        found      good   qualiti  ...  chef  habenero      dude   mailbox  \\\n",
      "0    0.043478  0.043478  0.043478  ...   0.0  0.000000  0.000000  0.000000   \n",
      "1    0.000000  0.000000  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "2    0.000000  0.000000  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "3    0.055556  0.055556  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...  ...   ...       ...       ...       ...   \n",
      "992  0.000000  0.000000  0.000000  ...   0.0  0.045455  0.000000  0.000000   \n",
      "993  0.000000  0.000000  0.000000  ...   0.0  0.000000  0.035714  0.035714   \n",
      "994  0.000000  0.080000  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "995  0.000000  0.000000  0.000000  ...   0.0  0.040000  0.000000  0.000000   \n",
      "996  0.000000  0.000000  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "\n",
      "         wing    squirt     jalap     ntild         o    sorbat  \n",
      "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "992  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "993  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "994  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "996  0.018868  0.018868  0.018868  0.018868  0.018868  0.018868  \n",
      "\n",
      "[997 rows x 4203 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for TF\n",
    "tf_df1 = pd.DataFrame(tf_data1).fillna(0)\n",
    "print(\"TF df1 Scores:\")\n",
    "print(tf_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "883339ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF df2 Scores:\n",
      "       bought   several  vitality    canned       dog      food   product  \\\n",
      "0    0.043478  0.043478  0.043478  0.043478  0.043478  0.043478  0.130435   \n",
      "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.111111   \n",
      "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "992  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "993  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "994  0.000000  0.000000  0.000000  0.000000  0.000000  0.040000  0.000000   \n",
      "995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "996  0.018868  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        found      good   quality  ...  habenero      dude   mailbox  \\\n",
      "0    0.043478  0.043478  0.043478  ...  0.000000  0.000000  0.000000   \n",
      "1    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "3    0.055556  0.055556  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "992  0.000000  0.000000  0.000000  ...  0.045455  0.000000  0.000000   \n",
      "993  0.000000  0.000000  0.000000  ...  0.000000  0.035714  0.035714   \n",
      "994  0.000000  0.080000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "995  0.000000  0.000000  0.000000  ...  0.040000  0.000000  0.000000   \n",
      "996  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "     suckered      wing    squirt    jalape    ntilde         o   sorbate  \n",
      "0        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "992      0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "993      0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "994      0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "995      0.04  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "996      0.00  0.018868  0.018868  0.018868  0.018868  0.018868  0.018868  \n",
      "\n",
      "[997 rows x 5133 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for TF\n",
    "tf_df2 = pd.DataFrame(tf_data2).fillna(0)\n",
    "print(\"TF df2 Scores:\")\n",
    "print(tf_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f5b3215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDF df1 Scores:\n",
      "      toxic   chedder       air   favorit      base    vendor     limit  \\\n",
      "0  6.211604  6.904751  4.825309  2.427414  4.132162  5.295313  4.602166   \n",
      "\n",
      "   distinct   heavili       bet  ...    earthi       hid    potent     favor  \\\n",
      "0  5.295313  6.211604  5.806138  ...  6.211604  6.211604  6.904751  4.602166   \n",
      "\n",
      "      mixer     becam      chop     bewar    arabia       ace  \n",
      "0  6.904751  4.958841  4.958841  5.518456  6.904751  6.904751  \n",
      "\n",
      "[1 rows x 4203 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute IDF - the number of columns should be the same as you calculate in tf\n",
    "idf1 = compute_idf(data_lemmatized1)\n",
    "idf_df1 = pd.DataFrame([idf1]).fillna(0)\n",
    "print(\"\\nIDF df1 Scores:\")\n",
    "print(idf_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d710b7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDF df2 Scores:\n",
      "       serf  restaurant   allowed  labeling  distinct    eludes  overheated  \\\n",
      "0  6.211604    4.419844  5.518456  6.904751  5.806138  6.904751    6.904751   \n",
      "\n",
      "     target   whacked   hotline  ...    nicely        lo   suspect      tiny  \\\n",
      "0  5.518456  6.904751  6.904751  ...  4.825309  6.904751  5.806138  4.339801   \n",
      "\n",
      "      buyer    potent  criticism   rapidly    arabia    mousie  \n",
      "0  5.518456  6.904751   6.904751  6.904751  6.904751  6.904751  \n",
      "\n",
      "[1 rows x 5133 columns]\n"
     ]
    }
   ],
   "source": [
    "idf2 = compute_idf(data_lemmatized2)\n",
    "idf_df2 = pd.DataFrame([idf2]).fillna(0)\n",
    "print(\"\\nIDF df2 Scores:\")\n",
    "print(idf_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0def289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF df1 Scores:\n",
      "       bought     sever     vital       can       dog      food   product  \\\n",
      "0    0.107562  0.153802  0.300207  0.200094  0.148184  0.080094  0.221121   \n",
      "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.188363   \n",
      "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "992  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "993  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "994  0.000000  0.000000  0.000000  0.000000  0.000000  0.073686  0.000000   \n",
      "995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "996  0.046678  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        found      good   qualiti  ...  chef  habenero      dude   mailbox  \\\n",
      "0    0.107048  0.059112  0.118711  ...   0.0  0.000000  0.000000  0.000000   \n",
      "1    0.000000  0.000000  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "2    0.000000  0.000000  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "3    0.136783  0.075532  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...  ...   ...       ...       ...       ...   \n",
      "992  0.000000  0.000000  0.000000  ...   0.0  0.282346  0.000000  0.000000   \n",
      "993  0.000000  0.000000  0.000000  ...   0.0  0.000000  0.246598  0.246598   \n",
      "994  0.000000  0.108766  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "995  0.000000  0.000000  0.000000  ...   0.0  0.248464  0.000000  0.000000   \n",
      "996  0.000000  0.000000  0.000000  ...   0.0  0.000000  0.000000  0.000000   \n",
      "\n",
      "         wing    squirt     jalap     ntild         o    sorbat  \n",
      "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "992  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "993  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "994  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "996  0.130278  0.130278  0.130278  0.130278  0.130278  0.130278  \n",
      "\n",
      "[997 rows x 4203 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute TF-IDF for each document\n",
    "tfidf_data1 = [compute_tfidf(doc, idf1) for doc in data_lemmatized1]\n",
    "\n",
    "# Create DataFrame for TF-IDF\n",
    "tfidf_df1 = pd.DataFrame(tfidf_data1).fillna(0)\n",
    "print(\"\\nTF-IDF df1 Scores:\")\n",
    "print(tfidf_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48398422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF df2 Scores:\n",
      "       bought  several  vitality    canned       dog      food   product  \\\n",
      "0    0.107562  0.15855  0.300207  0.222304  0.148184  0.080094  0.221836   \n",
      "1    0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.188972   \n",
      "2    0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3    0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "..        ...      ...       ...       ...       ...       ...       ...   \n",
      "992  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "993  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "994  0.000000  0.00000  0.000000  0.000000  0.000000  0.073686  0.000000   \n",
      "995  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "996  0.046678  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        found      good   quality  ...  habenero      dude   mailbox  \\\n",
      "0    0.107048  0.059797  0.118711  ...  0.000000  0.000000  0.000000   \n",
      "1    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "3    0.136783  0.076407  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "992  0.000000  0.000000  0.000000  ...  0.282346  0.000000  0.000000   \n",
      "993  0.000000  0.000000  0.000000  ...  0.000000  0.246598  0.246598   \n",
      "994  0.000000  0.110026  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "995  0.000000  0.000000  0.000000  ...  0.248464  0.000000  0.000000   \n",
      "996  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "     suckered      wing    squirt    jalape    ntilde         o   sorbate  \n",
      "0     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "992   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "993   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "994   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "995   0.27619  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "996   0.00000  0.130278  0.130278  0.130278  0.130278  0.130278  0.130278  \n",
      "\n",
      "[997 rows x 5133 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute TF-IDF for each document\n",
    "tfidf_data2 = [compute_tfidf(doc, idf2) for doc in data_lemmatized2]\n",
    "\n",
    "# Create DataFrame for TF-IDF\n",
    "tfidf_df2 = pd.DataFrame(tfidf_data2).fillna(0)\n",
    "print(\"\\nTF-IDF df2 Scores:\")\n",
    "print(tfidf_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ac764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
