{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad54576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries for text preprocessing\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb10c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import topic modelling as well\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13b2911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Qeme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qeme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Qeme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download nltk resourses\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "934d6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data for our testing\n",
    "documents = [\n",
    "    \"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
    "    \"Rafael Nadal Is Out of the Australian Open\",\n",
    "    \"Biden Announces Virus Measures\",\n",
    "    \"Biden's Virus Plans Meet Reality\",\n",
    "    \"Where Biden's Virus Plan Stands\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9afb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rafael', 'nadal', 'join', 'roger', 'federer', 'missing', 'open'],\n",
       " ['rafael', 'nadal', 'australian', 'open'],\n",
       " ['biden', 'announces', 'virus', 'measure'],\n",
       " ['biden', 'virus', 'plan', 'meet', 'reality'],\n",
       " ['biden', 'virus', 'plan', 'stand']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So lets do the preprocessing task for the documents\n",
    "# set up stopword from english language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# set up lemmatizer too\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower()) # tokenize first then we do lowercases\n",
    "    tokens = [token for token in tokens if token.isalnum()] #filter out non-alphanumeric tokens like numbers or special characters\n",
    "    tokens = [token for token in tokens if token not in stop_words] #now use the stop_words to remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] #now use the lemmatize as well\n",
    "    return tokens\n",
    "\n",
    "#preprocess each document in the list\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b9b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "#convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6467b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW RUN LDA\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
    "#so lets breakdown certain thing in this code\n",
    "#corpus: bag-of-words representation of the documents\n",
    "#num_topics: number of topics to be extracted by the model\n",
    "#id2word=dictionary : dictionary mapping from word IDs to words\n",
    "#passes: number of passes through the corpus during training - random assignment of the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c988b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now see the result\n",
    "\n",
    "#create an empty array to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "#iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    #for each document,convert to box representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    #get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    #determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x:x[1])[0]\n",
    "    #appendf to the list\n",
    "    article_labels.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e77f1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                             Article  Topic\n",
      "0  Rafael Nadal Joins Roger Federer in Missing U....      1\n",
      "1         Rafael Nadal Is Out of the Australian Open      1\n",
      "2                     Biden Announces Virus Measures      0\n",
      "3                   Biden's Virus Plans Meet Reality      0\n",
      "4                    Where Biden's Virus Plan Stands      0\n"
     ]
    }
   ],
   "source": [
    "#create dataframe to see them\n",
    "df = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "\n",
    "#Now print them to see the result\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13806856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"biden\" (weight: 0.166)\n",
      "- \"virus\" (weight: 0.166)\n",
      "- \"plan\" (weight: 0.119)\n",
      "- \"reality\" (weight: 0.071)\n",
      "- \"meet\" (weight: 0.071)\n",
      "- \"announces\" (weight: 0.071)\n",
      "- \"measure\" (weight: 0.071)\n",
      "- \"stand\" (weight: 0.071)\n",
      "- \"australian\" (weight: 0.024)\n",
      "- \"rafael\" (weight: 0.024)\n",
      "\n",
      "Topic 1:\n",
      "- \"open\" (weight: 0.131)\n",
      "- \"nadal\" (weight: 0.131)\n",
      "- \"rafael\" (weight: 0.131)\n",
      "- \"federer\" (weight: 0.079)\n",
      "- \"roger\" (weight: 0.079)\n",
      "- \"missing\" (weight: 0.079)\n",
      "- \"join\" (weight: 0.079)\n",
      "- \"australian\" (weight: 0.079)\n",
      "- \"virus\" (weight: 0.027)\n",
      "- \"biden\" (weight: 0.027)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prnt the top terms for each topic\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa620e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
